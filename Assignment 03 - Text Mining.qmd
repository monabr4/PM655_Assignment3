---
title: "Assignment 03 - Text Mining"
author: Mona Bandov
format:
  html:
    embed-resources: true
editor: visual
date: "2023-11-03"
---

## Assignment 03 - Text Mining

## **Due Date**

This assignment is due by midnight Pacific Time, November 3rd, 2023.

## **Text Mining**

A new dataset has been added to the data science data repository <https://github.com/USCbiostats/data-science-data/tree/master/03_pubmed>. The dataset contains 3,241 abstracts from articles collected via 5 PubMed searches. The search terms are listed in the second column, `term` and these will serve as the "documents." Your job is to analyse these abstracts to find interesting insights.

Loading the packages and the CSV file.

```{r}
library(stringr)
library(readr)
library(tm)
library(tokenizers)
library(plyr)
library(tidytext)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(forcats)
library(plyr)
library(magrittr)
pubmed <- read_csv("pubmed.csv")
head(pubmed)
```

1.  Tokenize the abstracts and count the number of each token. Do you see anything interesting? Does removing stop words change what tokens appear as the most frequent? What are the 5 most common tokens for each search term after removing stopwords?

When I tokenize the abstracts and count the number of each token, I see that the most common words are the stop words. The most common words are "the", "of", "and", "in," and "to". When I remove the stop words, the 5 most common tokens for each search words is "covid", "19", "patients", "cancer" and "prostate".

```{r}
#Tokenize the abstracts and count the number of each token.
tokens <-unnest_tokens(pubmed, abstract, output = token, token = "words")
token_counts <-table(tokens$token)
top_tokens <- head(sort(token_counts, decreasing = TRUE), 5)
top_tokens


#Does removing stop words change what tokens appear as the most frequent? 
tokens <-unnest_tokens(pubmed, abstract, output = token, token = "words")
token_counts <- table(tokens$token)
stop_words <- stopwords("en")
tokens <- tokens[!tokens$token %in% stop_words, ]
filtered_token_counts <- table(tokens$token)
top_tokens <- head(sort(filtered_token_counts, decreasing = TRUE), 5)
top_tokens

```

2.  Tokenize the abstracts into bigrams. Find the 10 most common bigrams and visualize them with ggplot2.

```{r}


tokens <-unnest_tokens(pubmed, abstract, output = token, token = "ngrams", n=2)
token_counts <-table(tokens$token)
top_tokens <- as.data.frame(head(sort(token_counts, decreasing = TRUE),10))

tokens_df <- as.data.frame(tokens)

filtered_tokens_df <- tokens_df %>%
  filter(token %in% top_tokens$Var1)


filtered_tokens_df$token <- factor(filtered_tokens_df$token, levels = top_tokens$Var1)

ggplot(filtered_tokens_df, aes(x = token)) +
  geom_bar(stat = "count", fill = "lightblue") + 
  labs(x = "Token", y = "Counts") +
  ggtitle("Top 10 Token Frequency Bar Plot") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

3.  Calculate the TF-IDF value for each word-search term combination (here you want the search term to be the "document"). What are the 5 tokens from each search term with the highest TF-IDF value? How are the results different from the answers you got in question 1?

"TF gives weight to terms that appear a lot, IDF gives weight to terms that appears in a few documents."

Instead of looking at each words as a whole, the table now separates the TD-IDF values by term. I noticed that based on the terms, the words are closely related to the topic of the term. This is because IDF ranks based on specificity and TF ranks on frequency. For example, term "Covid" has a high TD-IDF value for words such as "covid", "pandemic", "sars", and "cov". All of these values are words that are speicfic AND common in the term covid. Question 1 just counts the number of times a word occurs in the abstract.

```{r}


# calculating TI and IDF.
pubmed %>%
  unnest_tokens(word, abstract) %>%
  count(word, term) %>%
  bind_tf_idf(word, term, n) %>%
  arrange(term, desc(tf_idf)) %>%
  head(5)

# 5 common 
pubmed %>%
  unnest_tokens(word, abstract) %>%
  count(word, term) %>%
  bind_tf_idf(word, term, n) %>%
  arrange(term, desc(tf_idf)) %>% 
  group_by(term) %>%
  slice_head(n=5) 






```
